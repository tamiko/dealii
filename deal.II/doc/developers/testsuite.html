<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                 "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
  <title>The deal.II Testsuite</title>
  <link href="../screen.css" rel="StyleSheet">
  <meta name="author" content="the deal.II authors <authors@dealii.org>">
  <meta name="copyright" content="Copyright (C) 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013 by the deal.II authors">
  <meta name="date" content="$Date$">
  <meta name="svn_id" content="$Id$">
  <meta name="keywords" content="deal dealii finite elements fem triangulation">
  <meta http-equiv="content-language" content="en">
</head>
<body>


    <h1>The deal.II Testsuite</h1>

    <p>The deal.II testsuite consists of two parts, the
    <a href="#build_tests">build tests</a> and the
    <a href="#regression_tests">regression tests</a>. While the build tests
    just check if the
    library can be compiled on different systems and with different (versions
    of) compilers, the regression tests are actually run and their output
    compared with previously stored. These two testsuites are
    described below.</p>

    <a name="build_tests"></a>
    <h2>The build tests</h2>

    <p>
      With our build tests, we check if deal.II can be compiled on
      different systems and with different compilers as well as
      different configuration options. Results are collected in a
      database and can be accessed <a
      href="http://www.dealii.org/testsuite.html">online</a>.<p>

      <p>Running the build test suite is simple and we encourage deal.II
      users with configurations not found on the <a
      href="http://www.dealii.org/testsuite.html">test suite page</a> to
      participate. Assuming you checked out deal.II into the directory
      <code>dealtest</code>, running it is as simple as:
      <pre>

    cd dealtest
    svn update
    ./contrib/utilities/build_test
    mail build-tests@dealii.org &lt; *.log
  ( rm *.log )
      </pre>
    </p>

    <p>
      The <code>build_test</code> script supports the following options:
      <pre>

    SOURCEDIR     - the source directory to use (otherwise the current directory is used)
    CONFIGFILE    - A cmake configuration file for the build test
    LOGDIR        - directory for the log file
    LOGFILE       - the logfile to use, defaults to
                        $LOGDIR/$BRANCH.$CONFIGFILE.<unix time>.log

    CMAKE         - the cmake executable to use
    SVN           - svn info command to use, defaults to
                        svn info $(SOURCEDIR)
    TMPDIR        - defaults to "/tmp"
    CLEAN_TMPDIR  - defaults to "true"
    RUN_EXAMPLES  - defaults to "true"
      </pre>
      An example configuration file can be found <a
      href="../users/Config.sample">here</a>. Options can be passed either via
    environment
      <pre>

    export CONFIGFILE=MyConfiguration.conf
    ./contrib/utilities/build_test
      </pre>
      or directly on the command line:
      <pre>

    ./contrib/utilities/build_test CONFIGFILE=myConfiguration.conf
      </pre>
    </p>

    <p>
      A status indicator should appear on the build test website after some
      time (results are collected and processed by a program that is run
      periodically, but not immediately after a mail has been received).
    </p>


    <h3>Dedicated build tests</h3>

    There is a detailed example for dedicated build tests on the <a
      href="https://code.google.com/p/dealii/wiki/BuildTests">wiki</a>.

    <a name="regression_tests"></a>
    <h2>The regression tests</h2>

    <p>
    deal.II has a testsuite that, at the time this article is written (mid-2013),
    has some 2,900 small programs (growing by roughly one per day) that we run
    every time we make a
    change to make sure that no existing functionality is broken. The
    expected output is also stored in our subversion archive, and when you
    run a test you are notified if a test fails. These days, every
    time we add a significant piece of functionality, we add at least
    one new test to the testsuite, and we also do so if we fix a bug,
    in both cases to make sure that future changes do not break what
    we have just checked in. In addition, some machines run the tests
    every night and send the results back home; this is then converted
    into <a href="http://dealii.mathsim.eu/cgi-bin/regression_quick.pl"
    target="body">a webpage showing the status of our regression
    tests</a>.
    </p>

    <p>
    If you develop parts of deal.II, want to add something, or fix a
    bug in it, we encourage you to use our testsuite. This page
    documents some aspects of it.
    </p>


    <h3>Running it</h3>

    <p>
    To run the testsuite, go to the directory where you want to test deal.II
    and do this:
    <pre>

       svn checkout https://svn.dealii.org/trunk/tests
       cd tests
       DEAL_II_DIR=/a/b/c ./configure
    </pre>
    where <code>/a/b/c</code> is the installation directory you have told
    CMake to install deal.II into previously.
    </p>

    <p>
    Not all tests succeed on every machine even if all computations are
    correct, because you might not have configured with all the required
    packages (for example PETSc or Trilinos), or because your machine
    generates slightly different floating point outputs.  To increase the
    number of tests that work correctly, install the
    <a href="http://www.nongnu.org/numdiff/">numdiff</a> toold that compares
    stored and newly created output files based on floating point
    tolerances. To use it, simply export the environment variable
    <code>export DEAL_II_DIFF="numdiff -a 1e-6 -q"</code>
    before running the testsuite.
    </p>

    <p>
    Once you have done this, you may simply type
    <code>make</code>. This runs all the tests there are, but stops at
    the first one that fails to either execute properly or for which
    the output does not match the expected output found in the subversion
    archive. This is helpful if you want to figure out if any test is
    failing at all. Typical output looks like this:
    <pre>

      deal.II/tests> make
      cd base ; make
      make[1]: Entering directory `/ices/bangerth/p/deal.II/1/deal.II/tests/base'
      =====linking======= logtest.exe
      =====Running======= logtest.exe
      =====Checking====== logtest.output
      =====OK============ logtest.OK
      =====linking======= reference.exe
      =====Running======= reference.exe
      =====Checking====== reference.output
      =====OK============ reference.OK
      =====linking======= quadrature_test.exe
      ...
    </pre>
    Be aware that because of the number of tests we have, running the entire
    testsuite takes approximately 10 hours (as of early 2013), even on a fast
    system. (On the other hand, of couse only a large testsuite can offer
    comprehensive coverage of a software as big as deal.II.) This time can be
    reduced, however, on multicore machines if you use the command
    <code>make -jN</code> where <code>N</code> is an integer equal to or
    slightly larger than the number of processor cores you have, as this
    instructs <code>make</code> to run several tests at the same time.
    </p>

    <p>
    Sometimes, you know that for whatever reason one test
    always fails on your system, or has already failed before you made
    any changes to the library that could have caused tests to
    fail. We also sometimes check in tests that we know presently
    fail, just to remind us that we need to work on a fix, if we don't
    have the time to debug the problem properly right away. In this
    case, you will not want the testsuite to stop at the first test
    that fails, but will want to run all tests first and then inspect
    the output to find any fails. There are make targets for this
    as well. The usual way we use the testsuite is to run all tests
    like this
    (the same applies as above: <code>make -jN</code> can be used on multicore
    machines):
    <pre>

      deal.II/tests> make report | tee report
    </pre>
    which produces the file report ( here in the test directory <tt>a-framework</tt>)
    <pre>

      =====Checking====== miscompare/output
      +++++Error+++++++++ miscompare/OK (miscompare/cmp/generic) Use make verbose=on for the diffs
      =====linking======= compile/exe
      =====Running======= link/exe
      =====debug========= fail.cc
      make[1]: Leaving directory `/home/kanschat/deal/tests/a-framework'
      Revision: 21455
      Date:  2010 187 2010-07-06 27-2
      Id:  kanschat@odin
      2010-07-06 16:39  1   a-framework/compile
      2010-07-06 16:39  0   a-framework/fail
      2010-07-06 16:39  2   a-framework/link
      2010-07-06 16:39  3   a-framework/miscompare
      2010-07-06 16:39   +  a-framework/run
    </pre>
    The last lines are the ones we are looking for: they show the time at which
    the tests was run, an indicator of success, and the name of a
    test. The indicator is either a plus, which means that the test
    compiled and linked successfully and that the output compared
    successfully against the stored results. Otherwise, it is any of the
    numbers 0 to 3, indicating failure at different levels:
    <ul>
      <li> 0: compiling failed
      <li> 1: linking failed
      <li> 2: the program crashed
      <li> 3: output differs from stored result
      <li> +: test succeeded
    </ul>
    If you only want to see the tests that failed, after the previous command,
    issue
    <pre>

      grep -v + report
    </pre>
    </p>

    <p>
    If you want to do a little more than just that, you should
    consider running
    <pre>

      make report+mail | tee report
    </pre>
    instead. This does all the same stuff, but also mails the test
    result to our central mail result server which will in regular
    intervals (at least once a day) munge these mails and present them
    on our <a href="http://www.dealii.org/~archiver/cgi-bin/regression_quick.pl"
    target="body">test site</a>. This way, people can
    get an overview of what tests fail. You may even consider running
    tests nightly through a cron-job with this command, to have
    regular test runs.
    </p>

    <p>
    To get a quick overview you can run
    <pre>

      make report+summary
    </pre>
    instead. This runs all the tests and outputs a table in the following format
    at the end:
    <pre>

                Compiling Linking Running   Check      OK     all
         a-framework	1	1	1	1	1	5
                base	0	0	0	2	185	187
                 lac	0	0	0	0	117	117
                  fe	0	0	0	4	114	118
             deal.II	0	0	0	2	291	293
         integrators	0	0	0	0	15	15
           multigrid	0	0	0	0	35	35
		 ...
    </pre>
    </p>

    <p>
    If a test failed, you have to find out what exactly went
    wrong. For this, you will want to go into the directory of that
    test, and figure out in more detail what went wrong. For example,
    if above test <code>hierarchical</code> would have failed, you
    would want to go into the <code>base</code> directory (this is
    given in the line with the equals signs; there are tests in other
    directories as well) and then type
    <pre>

      make hierarchical/exe
    </pre>
    to compile and link the executable. (For each test there is a not
    only a file with suffic <code>.cc</code> but also a subdirectory with the
    same name, in which we store among other things the executable for that
    test, under the name <code>exe</code>.) If this fails, i.e. if
    you can't compile or link, then you probably already know where
    the problem is, and how to fix it. If you could compile and link
    the test, you will want to make sure that it executes correctly
    and produces an output file:
    <pre>

      make hierarchical/output
    </pre>
    (As you see, the output file is also stored in the subdirectory with the
    test's name.) If this produces errors or triggers assertions, then you will
    want to use a debugger on the executable to figure out what happens. On
    the other hand, if you are sure that this also worked, you will
    want to compare the output with the stored output from subversion:
    <pre>

      make hierarchical/OK
    </pre>
    If the output isn't equal, then you'll see something like
    this:
    <pre>

      =====Checking====== hierarchical/output
      +++++Error+++++++++ hierarchical/OK. Use make verbose=on for the diffs
    </pre>
    Because the diffs between the output we get and the output we
    expected can sometimes be very large, you don't get to see it by
    default. However, following the suggestion printed, if you type
    <pre>

      make hierarchical/OK verbose=on
    </pre>
    you get to see it all:
    <pre>

      =====Checking====== hierarchical/output
      12c12
      < DEAL::0.333 1.667 0.333 -0.889 0.296 -0.988 0.329 -0.999 0.333 -1.000 0.333 -1.000
      ---
      > DEAL::0.333 0.667 0.333 -0.889 0.296 -0.988 0.329 -0.999 0.333 -1.000 0.333 -1.000
      +++++Error+++++++++ hierarchical/OK
    </pre>
    In this case, the second number on line 12 is off by one. To find
    the reason for this, you again should use a debugger or other
    suitable means, but that of course depends on what changes you
    have made last and that could have caused this discrepancy.
    </p>



    <h3>Adding new tests</h3>

    <p>
    As mentioned above, we add a new test every
    time we add new functionality to the library or fix a bug. If you
    want to contribute code to the library, you should do this
    as well. Here's how: you need a testcase,
    a subdirectory with the same name as the test, and a file with the
    expected output.
    </p>

    <h4>The testcase</h4>
    <p>
    For the testcase, we usually start from a template like this:
    <pre>

// ---------------------------------------------------------------------
// $Id$
//
// Copyright (C) 2013 by the deal.II authors
//
// This file is part of the deal.II library.
//
// The deal.II library is free software; you can use it, redistribute
// it, and/or modify it under the terms of the GNU Lesser General
// Public License as published by the Free Software Foundation; either
// version 2.1 of the License, or (at your option) any later version.
// The full text of the license can be found in the file LICENSE at
// the top level of the deal.II distribution.
//
// ---------------------------------------------------------------------


// a short (a few lines) description of what the program does

#include "../tests.h"
#include <iostream>
#include <fstream>

// all include files you need here


int main ()
{
  std::ofstream logfile("my_new_test/output");
  deallog.attach(logfile);
  deallog.depth_console(0);

  // your testcode here:
  int i=0;
  deallog << i << std::endl;

  return 0;
}
    </pre>

    <p>You open an output file in a directory with the same
    name as your test, and then write
    all output you generate to it,
    through the <code>deallog</code> stream.   The <code>deallog</code>
    stream works like any
    other <code>std::ostream</code> except that it does a few more
    things behind the scenes that are helpful in this context. In
    above case, we only write a zero to the output
    file. Most tests actually write computed data to the output file
    to make sure that whatever we compute is what we got when the
    test was first written.
    </p>

    <p>
    There are a number of directories where you can put a new test.
    Extensive tests of individual classes or groups of classes
    have traditionally been into the <code>base/</code>,
    <code>lac/</code>, <code>deal.II/</code>, <code>fe/</code>,
    <code>hp/</code>, or <code>multigrid/</code> directories, depending on
    where the classes that are tested are located.
    </p>

    <p>
    We have started to create more atomic tests which
    are usually very small and test only a single aspect of the
    library, often only a single function. These tests go into the
    <code>bits/</code> directory and often have names that are
    composed of the name of the class being tested and a two-digit
    number, e.g., <code>dof_tools_11</code>. There are
    directories for PETSc and Trilinos wrapper functionality.
    </p>

    <h4>A directory with the same name as the test</h4>

    <p> You have to create a subdirectory
    with the same name as your test to hold the output from the test.

    <p> One convenient way to create this subdirectory with the correct
    properties is to use svn copy.
    <pre>

    svn copy existing_test_directory my_new_test
    </pre>

    <p>
    Once you have done this, you can try to run
    <pre>

      make my_new_test/output
    </pre>
    This should compile, link, and run your test. Running your test
    should generate the desired output file.
    </p>



    <h4>An expected output</h4>

    <p>
    If you run your new test executable, you will get an output file
    <code>mytestname/output</code> that should be used to compare all future
    runs with. If the test
    is relatively simple, it is often a good idea to look at the
    output and make sure that the output is actually what you had
    expected. However, if you do complex operations, this may
    sometimes be impossible, and in this case we are quite happy with
    any reasonable output file just to make sure that future
    invokations of the test yield the same results.
    </p>

    <p>
    The next step is to copy this output file to the place where the
    scripts can find it when they compare with newer runs. For this, you first
    have to understand how correct results are verified. It works in the
    following way: for each test, we have subdirectories
    <code>testname/cmp</code> where we store the expected results in a file
    <code>testname/cmp/generic</code>. If you create a new test, you should
    therefore create this directory, and copy the output of your program,
    <code>testname/output</code> to <code>testname/cmp/generic</code>.
    </p>

    <p>
    Why <code>generic</code>? The reason is that sometimes test results
    differ slightly from platform to platform, for example because numerical
    roundoff is different due to different floating point implementations on
    different CPUs. What this means is that sometimes a single stored output is
    not enough to verify that a test functioned properly: if you happen to be
    on a platform different from the one on which the generic output was
    created, your test will always fail even though it produces almost exactly
    the same output.
    </p>

    <p>
    To avoid this, what the makefiles do is to first check whether an output
    file is stored for this test and your particular configuration (platform
    and compiler). If this isn't the case, it goes through a hierarchy of files
    with related configurations, and only if none of them does it take the
    generic output file. It then compares the output of your test run with the
    first file it found in this process. To make things a bit clearer, if you
    are, for example, on a <code>i686-pc-linux-gnu</code> box and use
    <code>gcc4.0</code> as your compiler, then the following files will be
    sought (in this order):
    <pre>

testname/cmp/i686-pc-linux-gnu+gcc4.0
testname/cmp/i686-pc-linux-gnu+gcc3.4
testname/cmp/i686-pc-linux-gnu+gcc3.3
testname/cmp/generic
    </pre>
    (This list is generated by the <code>tests/hierarchy.pl</code> script.)
    Your output will then be compared with the first one that is actually
    found. The virtue of this is that we don't have to store the output files
    from all possible platforms (this would amount to gigabytes of data), but
    that we only have store an output file for gcc4.0 if it differs from that
    of gcc3.4, and for gcc3.4 if it differs from gcc3.3. If all of them are the
    same, we would only have the generic output file.
    </p>

    <p>
    Most of the time, you will be able to generate output files only
    for your own platform and compiler, and that's alright: someone
    else will create the output files for other platforms
    eventually. You only have to copy your output file to
    <code>testname/cmp/generic</code>.
    </p>

    <p>
    At this point you can run
    <pre>

      make my_new_test/OK
    </pre>
    which should compare the present output with what you have just
    copied into the compare directory. This should, of course,
    succeed, since the two files should be identical.
    </p>

    <p>
    On the other hand, if you realize that an existing test fails on your
    system, but that the differences (as shown when running with
    <code>verbose=on</code>, see above) are only marginal and around the 6th or
    8th digit, then you should check in your output file for the platform you
    work on. For this, you could copy <code>testname/output</code> to
    <code>testname/cmp/myplatform+compiler</code>, but your life can be easier
    if you simply type
    <pre>

      make my_new_test/ref
    </pre>
    which takes your output and copies it to the right place automatically.
    </p>




    <h4>Checking in</h4>

    <p>
    Tests are a way to make sure everything keeps working. If they
    aren't automated, they are no good. We are therefore very
    interested in getting new tests. If you have subversion write access
    already, you can add the new test and the expected output
    file:
    <pre>

      svn add bits/my_new_test.cc
      svn add bits/my_new_test
      svn add bits/my_new_test/cmp
      svn add bits/my_new_test/cmp/generic
      svn commit -m "New test" bits/my_new_test*
    </pre>
    In addition, you should do the following in order to avoid that the files
    generated while running the testsuite show up in the output of <code>svn
    status</code> commands:
    <pre>

      svn propset svn:ignore "obj.*
        exe
        output
        status
        OK" bits/my_new_test
      svn commit -m "Ignore generated files." bits/my_new_test
    </pre>
    Note that the list of files given in quotes to the propset command extends
    over several lines.
    </p>

    <p>
    If you don't have subversion write access, talk to us in the discussion group;
    writing testcases is a worthy and laudable task, and we would
    like to encourage it by giving people the opportunity to
    contribute!
    </p>

    <hr />
    <address>
      <a href="../authors.html" target="body">The deal.II Authors</a>
      $Date$
    </address>
    <div class="right">
      <a href="http://validator.w3.org/check?uri=referer" target="_top">
        <img style="border:0" src="http://www.w3.org/Icons/valid-html401" alt="Valid HTML 4.01!"></a>
      <a href="http://jigsaw.w3.org/css-validator/check/referer" target="_top">
        <img style="border:0;width:88px;height:31px" src="http://jigsaw.w3.org/css-validator/images/vcss" alt="Valid CSS!"></a>
    </div>

  </body>
</html>
